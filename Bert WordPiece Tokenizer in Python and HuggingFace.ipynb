{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "25180c08-2f26-4ee0-a74c-89f65542a057",
   "metadata": {},
   "source": [
    "## Run the following pip commands if necessary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a42cfd02-831c-4476-b350-1c128e5e371f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fe6a8c2c-c36c-4c94-8c14-239397c9ac20",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install tokenizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9ae0ae93-d8d0-419f-859a-c04b8cf92cf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6ef3a31-70c5-42a0-bca4-c8358cfa335a",
   "metadata": {},
   "source": [
    "## Create directories to store dataset and tokenizer files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4a19664a-4e84-412c-98d3-9bc17021e66d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.mkdir(\"./oscar\")\n",
    "os.mkdir(\"./new_tokenizer\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd6b92c3-a348-4e91-a310-16eaf72eaef1",
   "metadata": {},
   "source": [
    "## Select the dataset we want to use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "86923b7e-af87-4cda-95d1-2909117d42d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "dataset = load_dataset(\"nthngdy/oscar-small\", \"unshuffled_deduplicated_en\",split=\"train\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fdbe3cb-c5ca-4e20-94f0-e18d0e6c97ef",
   "metadata": {},
   "source": [
    "## Break up data into smaller files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "059d41ba-1613-43d9-a4fb-0571aa83423c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d9365923e55c49eda1d5fc789d476a19",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/595810 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from tqdm.auto import tqdm\n",
    "\n",
    "text_data = []\n",
    "file_count = 0\n",
    "for sample in tqdm(dataset):\n",
    "    sample = sample[\"text\"].replace(\"\\n\", \" \")\n",
    "    text_data.append(sample)\n",
    "    # This will save data into multiple files\n",
    "    if len(text_data) == 5000:\n",
    "        with open(f\"./oscar/file_{file_count}.txt\", \"w\", encoding=\"utf-8\") as fp:\n",
    "            fp.write(\"\\n\".join(text_data))\n",
    "        text_data = []\n",
    "        file_count += 1\n",
    "# This will save whatever data is left into a file\n",
    "with open(f\"./oscar/file_{file_count}.txt\", \"w\", encoding=\"utf-8\") as fp:\n",
    "    fp.write(\"\\n\".join(text_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73ceed0b-5d18-44e8-b06f-2c4d80dbe7e0",
   "metadata": {},
   "source": [
    "## Get list of files we will feed the tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e98314f8-241a-4f3e-959c-8901837d9cd1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['oscar\\\\file_0.txt',\n",
       " 'oscar\\\\file_1.txt',\n",
       " 'oscar\\\\file_10.txt',\n",
       " 'oscar\\\\file_100.txt',\n",
       " 'oscar\\\\file_101.txt']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "paths = [str(x) for x in Path(\"./oscar\").glob(\"**/*.txt\")]\n",
    "paths[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "42dfdb8d-f540-4f5b-a4d5-f745fe287e0c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "120"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(paths)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "430d9f4f-34a5-4c0f-bf53-b7f208bf3ac4",
   "metadata": {},
   "source": [
    "## Initialize tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "21d3b142-d8ed-44c0-8ba5-d4f3024bf4b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizers import BertWordPieceTokenizer\n",
    "tokenizer = BertWordPieceTokenizer(\n",
    "    clean_text=True, # Removes obvious characters that we don't want. Converts all white space into spaces\n",
    "    handle_chinese_chars=False, # It allows chinese characters to be better represented\n",
    "    strip_accents=False, # Gets rid of accents in languages like spanish (Ã© -> e)\n",
    "    lowercase=True # Convert uppercase letters to lowercase\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8678b1c8-eb02-4786-b987-db8eaeb944a9",
   "metadata": {},
   "source": [
    "## Train tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "98e0e69d-2081-4438-8b2b-c1e9ad50946f",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.train(files = paths,\n",
    "                vocab_size=10000, # Number of tokens we can have in our tokenizer\n",
    "                min_frequency=2, # Minimum number of times we must see 2 different tokens or characters\n",
    "                                 # together in order to be considered as a token by themselves\n",
    "                special_tokens=[\"[PAD]\", \"[UNK]\", \"[CLS]\", \"[SEP]\", \"[MASK]\"], # Special tokens used by Bert\n",
    "                limit_alphabet=1000, # number of single character tokens that we can see within our vocab\n",
    "                wordpieces_prefix=\"##\" # Indicates a piece of a word rather than a full word\n",
    "               )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d6cf252-5090-40d6-9d68-c462e47afb5d",
   "metadata": {},
   "source": [
    "## Save tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "57ac5554-0819-4463-bb3f-95298a1ef934",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['./new_tokenizer\\\\vocab.txt']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.save_model(\"./new_tokenizer\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95fd7484-1f71-45c5-9bf0-ac1712f36628",
   "metadata": {},
   "source": [
    "## Load Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "61e16663-ca51-4a24-8ae1-3e301db9c5cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "None of PyTorch, TensorFlow >= 2.0, or Flax have been found. Models won't be available and only tokenizers, configuration and file/data utilities can be used.\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained(\"./new_tokenizer\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "426b6a13-bd43-4172-89cd-6df4ea6e1302",
   "metadata": {},
   "source": [
    "## Tokenize a sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a3a77658-c053-45ea-933b-96cc92a3f3bc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [2, 6684, 5, 2195, 2017, 1962, 35, 3], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1]}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer(\"hello! how are you?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67bbd1a8-8449-412e-99b3-96f698596c41",
   "metadata": {},
   "source": [
    "## Split tokenizer words and save into variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ee089de5-2d6f-43ee-844a-ca2e4fa00488",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./new_tokenizer/vocab.txt\", \"r\", errors=\"ignore\") as fp:\n",
    "    vocab = fp.read().split(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b2719a15-9495-4319-81b5-ac4bdb5d3867",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[PAD]', '[UNK]', '[CLS]', '[SEP]', '[MASK]']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ee386fd-a2a3-4260-b3a5-d2c4af7c5280",
   "metadata": {},
   "source": [
    "## Get the token values from a sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9936fc8f-b750-4ac9-a650-6e2b16af4134",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2, 6684, 5, 2195, 2017, 1962, 35, 3]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer(\"hello! how are you?\")[\"input_ids\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4be00f25-37cf-4b53-b444-df8f3ae37368",
   "metadata": {},
   "source": [
    "## Recreate a sentence with tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6b41a4ac-ba75-4cef-9593-7c09d24f250a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CLS] hello ! how are you ? [SEP] "
     ]
    }
   ],
   "source": [
    "for i in tokenizer(\"hello! how are you?\")[\"input_ids\"]:\n",
    "    print(vocab[i], end=\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a48b040f-2410-44fc-83f0-7993a9d16cd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CLS] i understood nothing [SEP] "
     ]
    }
   ],
   "source": [
    "for i in tokenizer(\"I understood nothing\")[\"input_ids\"]:\n",
    "    print(vocab[i], end=\" \")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "563ad4ad-e055-465f-b7e5-0311bdcd4c97",
   "metadata": {},
   "source": [
    "## Use a word that will have pieces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0ea99654-b827-471d-b21a-ebf2e2e3e390",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CLS] respons ##ability [SEP] "
     ]
    }
   ],
   "source": [
    "for i in tokenizer(\"responsability\")[\"input_ids\"]:\n",
    "    print(vocab[i], end=\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d9ed40c-af07-4bd2-9630-e0a0b9e8fd83",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
